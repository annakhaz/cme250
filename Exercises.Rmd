---
title: "CME 250 HW"
author: "Anna Khazenzon"
date: "February 16, 2015"
output: html_document
---

#Conceptual Exercises

##(Introduction) section 2.4

### Question 1

#### Part a
If the sample size n is extremely large, and the number of predictors p is small, we would expect a flexible method to work better. A flexible method is unlikely to overfit the data with a large n, and would instead better fit the data than an inflexible model would.

#### Part b
If the number of predictors p is extremely large, and the number of observations is small, we would expect an inflexible method to work better. A flexible method would likely overfit the data.

#### Part c
If the relationship between the predictors and response is highly
non-linear, we would expect a flexible method to work better. Flexible methods are able to reveal more complex shapes than inflexible methods (such as linear regression).

#### Part d
If the variance of the error terms, i.e. σ2 = Var(), is extremely
high, we would expect an inflexible method to work better. A flexible method would be likely to follow the errors too closely, and thus overfit the data.

### Question 2

#### Part a
(a) We collect a set of data on the top 500 firms in the US. For each
firm we record profit, number of employees, industry and the
CEO salary. We are interested in understanding which factors
affect CEO salary.

Regression, inference

n=500 (top firms)
p=3 (profit, number of employes, industry)

#### Part b
(b) We are considering launching a new product and wish to know
whether it will be a success or a failure. We collect data on 20
similar products that were previously launched. For each product
we have recorded whether it was a success or failure, price
charged for the product, marketing budget, competition price,
and ten other variables.

Classification, prediction

n=20 (similar products)
p=13 (price charged, marketing budget, competition price, +10)

#### Part c
(c) We are interested in predicting the % change in the US dollar in
relation to the weekly changes in the world stock markets. Hence
we collect weekly data for all of 2012. For each week we record
the % change in the dollar, the % change in the US market,
the % change in the British market, and the % change in the German market.

Regression, prediction

n=52 (weeks)
p=3 (% change in dollar, % change in US market, % change in British market, % change in German market)

### Question 6

A parametric statistical learning approach involves constructing a model that can be parameterized by a finite number of parameters. Parametric procedures rely on assumptions about the shape of the distribution in the underlying population, and about the form, or parameters, of the assumed distribution. On the other hand, a nonparametric model cannot be parameterized by a fixed number of parameters. 

An advantage of nonparametric methods over parametric is that they can handle nonnormal data, because they do not rely on assumptions about the shape or form of the probability distribution from which the data are drawn. Using a parametric approach to analyze data which do not meet underlying assumptions can lead to misleading results.

A disadvantage of nonparametric approaches is that they can be less powerful than their paramatric counterparts when the data truly are normally distributed, so there is greater incidence of Type II errors, or false negatives. A larger sample size will be necessary to rectify this issue. Another disadvantage is that results can be more difficult to interpret.

More specifically, parametric statistical learning approaches involve a two-step model-based approach: first we make an assumption about the shape of the function $f$ to select a model, then we use training data to fit the model. This approach is called parametric because it reduces the problem of estimating the function down to one of estimating a set of parameters. An advantage of this approach is that it simplifies the problem of estimating $f$ with a set of parameters, rather than fitting an entirely arbitrary $f$. The disadvantage is that the model we choose might not match the true unknown form of $f$. To ameliorate this problem, we might fit a more flexible model, but this may lead to overfitting of the data (ie following the noise in the data too closely). 

Nonparametric statistical learning approaches do not make explicit assumptions about the functional form of $f$, but rather seek an estimate for $f$ that is reasonably close. Without relying on assumptions, nonparametric approaches can accurately fit a wider range of possible shapes for $f$.

##(Linear Regression)  section 3.7

### Question 3

#### Part a

iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.

Even with a fixed value of IQ and GPA, because of the interaction effect between GPA and gender, males earn more on average than females at a high GPA, eg a 4.0.

#### Part b

$$y = 50 + 20*4 + .07*110 + 35*1 + .01*110*4 + (-10)*4*1 = 137.1$$

The predicted starting salary for a female with IQ 110 and GPA 4.0 is $137,100.

##(Logistic Regression) section 4.7

### Question 8

Based on these results, we should prefer to use logistic regression for classification. Even though 1-nearest neighbors has a lower average error rate than either training or test error rates for logistic regression, this is misleading, because 1-nearest neighbors is drastically overfitting the data. It is unlikely that it would do well on a training set. The classifier likely had 0% error at training, and 36% error at test, because the decision boundary is fit closely to every single data point, and would not generalize well to other data.

##(Regularization) section 6.8

### Question 2

#### Part a

The lasso, relative to least squares, is less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. With a higher tuning parameter lambda, lasso becomes less flexible. Lasso only incorporates predictors with nonzero coefficients in the final model, so there are fewer predictors. It becomes less prone to overfitting.

#### Part b

Ridge regression, relative to least squares, is less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. As the tuning parameter for lambda increases, ridge regression becomes less flexible. When lambda is equal to zero, it is equivalent to least squares.  

#Applied Exercises

``` {r load packages}
library(ggplot2)
```
##(Introduction) section 2.4

### Question 9 

#### Part a

``` {r load auto data}
a <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Auto.csv", header=T)
```

Quantitative parts of the predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year
Qualitative parts of the predictors: origin, name

#### Part b

Range of quantitative predictors:

```{r} 
summary(a)
```

mpg: 9-46.6
cylinders: 3-8
displacement: 68-455
horsepower: 75-150
weight: 1613-5140
acceleration: 8-24.8
year: 70-82

#### Part c

Mean and standard deviation of quantitative predictors:

```{r echo=FALSE}
print("mean mpg: ", quote=FALSE)
mean(a$mpg)
print("mpg sd: ", quote=FALSE)
sd(a$mpg)

print("mean cylinders: ", quote=FALSE)
mean(a$cylinders)
print("cylinders sd: ", quote=FALSE)
sd(a$cylinders)

print("mean displacement: ", quote=FALSE)
mean(a$displacement)
print("displacement sd: ", quote=FALSE)
sd(a$displacement)

print("mean horsepower: ", quote=FALSE)
mean(as.numeric(a$horsepower))
print("horsepower sd: ", quote=FALSE)
sd(as.numeric(a$horsepower))

print("mean weight: ", quote=FALSE)
mean(a$weight)
print("weight sd: ", quote=FALSE)
sd(a$weight)

print("mean acceleration: ", quote=FALSE)
mean(a$acceleration)
print("acceleration sd: ", quote=FALSE)
sd(a$acceleration)

print("mean year: ", quote=FALSE)
mean(a$year)
print("year sd: ", quote=FALSE)
sd(a$year)
```

#### Part d

```{r}
#remove 10th-85th obs
a_subset <- a[10:85,]

#range & mean
summary(a_subset)

#MPG
sd(a_subset$mpg)

#Cylinders
sd(a_subset$cylinders)

#Displacement
sd(a_subset$displacement)

#Horsepower
sd(as.numeric(a_subset$horsepower))

#Weight
sd(a_subset$weight)

#Acceleration
sd(a_subset$acceleration)

#Year
sd(a_subset$year)

```

#### Part e

``` {r}
#plot data (scatter)
ggplot(a,aes(x=year,y=mpg)) + geom_jitter() + stat_smooth(method="lm")

ggplot(a,aes(x=acceleration,y=mpg)) + geom_point() + stat_smooth(method="lm")

ggplot(a,aes(x=weight,y=mpg)) + geom_point() + stat_smooth(method="lm")

ggplot(a,aes(x=displacement,y=mpg)) + geom_point() + stat_smooth(method="lm")

ggplot(a,aes(x=cylinders,y=mpg)) + geom_jitter() + stat_smooth(method="lm")

ggplot(a,aes(x=as.numeric(horsepower),y=mpg)) + geom_jitter() + stat_smooth(method="lm")  

ggplot(a,aes(x=origin,y=mpg)) + geom_bar(stat="identity")

```

Year, acceleration, horsepower appear to have a positive relationship with mpg. Weight, displacement, cylinders appear to have a negative relationship with mpg. American cars have higher mpg than European or Japanese.

#### Part f

The plot of effect of year on mpg suggests that more recent models of cars have higher miles per gallon, because we see an increasing slope across time.
acceleration

The plot of the effect of weight on mpg suggests that heavier cars have lower miles per gallon. This is plausible, since it would take more gasoline to move a heavier car.

##(Linear Regression)  section 3.7

### Question 9

#### Part a
``` {r}
pairs(a)
```

#### Part b
``` {r}
a_num <- a[,1:7]
a_num$horsepower <- as.numeric(a_num$horsepower)
cor(a_num)
```

#### Part c

``` {r}
rs <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year, data=a_num)
summary(rs)
```

There is a significant negative relationship between a car's weight and its mpg, _t_(390)=-11.43, _p_<.001. Converseley, there is a significant positive relationship between the year a car was made and its mpg, meaning modern cars have higher mpg, _t_(390)=15.025, _p_<.001

#### Part d

``` {r}
#diagnostic plots of linear reg fit
plot(rs)
```

The residual plot shows heteroscedasticity, as there is a dependency between the residuals and the fitted values, indicating that our data could be fit better with another model. Data points 323, 326, and 327 appear to be large outliers, and the leverage plot seems to identify point 14 as having unusually high leverage.

#### Part e

``` {r}
rs1 <- lm(mpg ~ weight * acceleration, data=a_num)
summary(rs1)

rs2<- lm(mpg ~ weight * origin, data=a)
summary(rs2)
```

There is a significant interaction between weight and acceleration in predicting mpg. There is also a significant interaction between weight and origin in predicting mpg.

#### Part f

``` {r}
rs1 <- lm(mpg ~ log(displacement) + displacement, data=a_num)
summary(rs1)

rs2 <- lm(mpg ~ poly(weight, 2), data=a_num)
summary(rs2)
```

There is a significant effect of log displacement on mpg. There is also a significant quadratic effect of weight on mpg, indicating that there might be a nonlinear relationship between how heavy a car is and how gas-efficient it is.

### Question 13

#### Part a

``` {r}
set.seed(2)

x <- rnorm(n = 100, mean = 0, sd = 1)
```

#### Part b

``` {r}
eps <- rnorm(n = 100, mean = 0, sd = .25)
```

#### Part c

``` {r}
y <- -1 + .5 * x + eps
```

Vector y has length 100; there are as many values of y as there are of x. In this linear model B0 is -1, and B1 is 0.5.

#### Part d

``` {r}
plot(x, y)
```

There appears to be a strong positive linear relationship between x and y. 

#### Part e

``` {r}
rs <- lm(y ~ x)
summary(rs)
```

The observed least squares linear model shows a highly significant positive relationship between x and y. In this model, B0 is -1.01 and B1 is .500, nearly identical to the B0 and B1 above.

#### Part f

``` {r}
plot(x, y)
abline(rs)
abline(a = -1, b = .5, col = "red")
legend("bottomright", lty=1, c("least squares", "population"), col=c(1,2))
```

#### Part g

``` {r}
rs_poly <- lm(y ~ poly(x,2))
summary(rs_poly)

anova(rs, rs_poly)
```

There is no evidence that the quadratic term improves the model fit. The quadratic term $x^2$ is not significantly predictive of $y$, _t_(97) = -1.40), _p_ = 0.164. When comparing the linear and polynomial models directly, we find that the model is not significantly improved, _F_(1, 97) = 1.97, _p_ = 0.164.

##(Regularization and Cross-validation) section 6.8

### Question 9

#### Part a

``` {r}
library(glmnet)
library(ISLR)

#going to predict number applications received given other factors

x = model.matrix(Apps ~ ., College)[,-1]
y = College$Apps
dim(College) # 777 rows, 18 cols

#split into training/test
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
```

#### Part b

``` {r}
summary(lm(y ~ x, subset = train))
lm.mod = glmnet(x[train,], y[train], alpha=0, lambda=0, thresh=1e-12)
lm.pred = predict(lm.mod, s=0, newx=x[test,], exact=T)
mean((lm.pred-y.test)^2) #1075351
```

The test error obtained by fitting a linear model using least squares on the training set is 1075351.

#### Part c

``` {r}
grid = 10^seq(10,-2,length=100) 
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
plot(ridge.mod)

set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam = cv.out$lambda.min; bestlam #411

# what is the test MSE assoc w/ lambda = 411?
ridge.pred = predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2) #1043745
```

With a λ of 411 chosen by cross-validation, we fit a ridge regression model on the training set, and obtain test error 1043745, lower than the MSE of our linear model using least squares.

#### Part d

(d) Fit a lasso model on the training set, with λ chosen by crossvalidation.
Report the test error obtained, along with the number
of non-zero coefficient estimates.

``` {r}
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min; bestlam #27
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2) #MSE 104082

out = glmnet(x ,y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17,]
lasso.coef
lasso.coef[lasso.coef != 0]
```

Using a LASSO model with a λ of 27 chosen by cross-validation, we obtain a slightly lower test error -- 104082. However, we now have fewer non-zero coefficient estimates -- 14 instead of 17.

##(Clustering and PCA) section 10.7

### Question 10

#### Part a

``` {r}
set.seed(3)
x = matrix(rnorm(60*2), ncol=2)
x[1:20,1] = x[1:20,1] + 6
x[1:20,2] = x[1:20,2] - 1
x[21:40,1] = x[21:40,1] - 7
x[21:40,2] = x[21:40,2] + 3
x[41:60,2] = x[41:60,2] - 5

plot(x)
```

#### Part b

``` {r}
x_clus = c(rep(1,20), rep(2,20), rep(3,20))
pr.out = prcomp(x)
biplot(pr.out)
plot(predict(pr.out), col=x_clus)
```

#### Part c

``` {r}
set.seed(4)
km.out = kmeans(x, 3, nstart = 20)
km.out
plot(x, col=(km.out$cluster + 1), main="K-Means Clustering results with K=3", xlab="", ylab="", pch=20, cex=2)
```

The K-means cluster labels are identical to the true class labels.

#### Part d

``` {r}
set.seed(4)
km.out = kmeans(x, 2, nstart = 20)
km.out
plot(x, col=(km.out$cluster + 1), main="K-Means Clustering results with K=2", xlab="", ylab="", pch=20, cex=2)
```

With K = 2, K-means clustering now clusters the two closest clusters  together.

#### Part e

``` {r}
set.seed(4)
km.out = kmeans(x, 4, nstart = 20)
km.out
plot(x, col=(km.out$cluster + 1), main="K-Means Clustering results with K=4", xlab="", ylab="", pch=20, cex=2)
```

With K = 4, K-means clustering still clusters two of the true clusters correctly, but it splits one cluster into two based on a boundary on the y-axis. 

#### Part f

``` {r}
set.seed(4)
km.out = kmeans(pr.out$x, 3, nstart = 20)
km.out
plot(x, col=(km.out$cluster + 1), main="K-Means Clustering results with K=3", xlab="", ylab="", pch=20, cex=2)
```

The results for K-means clustering with K = 3 on the first two principal component score vectors is identical to that with the raw data, because the principal component score vectors perfectly labeled the three clusters.

#### Part g

``` {r}
set.seed(4)
km.out = kmeans(scale(x, scale=T), 3, nstart = 20)
km.out
plot(x, col=(km.out$cluster + 1), main="K-Means Clustering results with K=3, scaled", xlab="", ylab="", pch=20, cex=2)
```

Results appear similar to those obtained in (b). Because our data were drawn from a normal distribution with an SD of 1, it is unsurprising that clustering should not change upon scaling.
